{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refining Raw Audio\n",
    "\n",
    "In this tutorial, you will learn how to apply `VocalForge.audio` pipelines on audio files.\n",
    "\n",
    "Each pipeline will (or at least attempt to) remove poor/inappropiate audio from each file in order to better prime it for dataset creation, or whatever other purpose you have in mind. These can be done in different order, or some not at all. It's up to you!\n",
    "\n",
    "The models generally consist of a neural network designed to identify a specific piece of audio, then mark timestamps for its removal. Let's go over the ones currently supported in order to better illistrates VocalForge's usefulness:\n",
    "\n",
    "- `Voice Detection` will remove segments of audio in which no human sounds are found. Say there is a long segment of city noise, or a musical intro to a podcast, all of this is removed. This is helpful not only in that it removes any of that non human audio, but it also reduces the time in which the subsequent audio takes to process.\n",
    "\n",
    "- `Overlap` covers speech that has two or more people talking at the same time. Not only does it forceably remove egotistical people from trying to take over a conversation, but it *also* removes poor audio from podcasts or other casual conversational settings.\n",
    "\n",
    "- `Isolate` one of the less straightforward pipelines, it goes through and seperates each speaker in each audio file. From there, you as a user can specify a specific speaker you want to target and it will find that same user across each audio file, even in different recording enviroments, such as a recording studio and a park. \n",
    "\n",
    "- `Export` is really just to put everything in a nice little bow. Given a directory, it will format on sample rate, as well as optionally normalize and noise reduce the audio. \n",
    "\n",
    "More pipelines are coming soonâ„¢"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: It is highly reccomended to run this on a conda enviroment if running locally by running the command\n",
    "`conda create -n VocalForge python=3.8 pytorch=1.11.0 torchvision=0.12.0 torchaudio=0.11.0 cudatoolkit=11.3.1 -c pytorch`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get to creating our work directory and installing `VocalForge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rio/Desktop/VocalForgeDev/VocalForge\n",
      "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
      "/bin/bash: -c: line 1: `mkdir os.path.join(root_path, 'work')'\n",
      "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
      "/bin/bash: -c: line 1: `mkdir os.path.join(root_path, 'work/audio')'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "root_path = os.getcwd()\n",
    "print(root_path)\n",
    "os.mkdir(os.path.join(root_path, 'work'))\n",
    "os.mkdir(os.path.join(root_path, 'work/audio'))\n",
    "work_path = os.path.join(root_path, 'work/audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this might take a while\n",
    "!pip install VocalForge['audio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rio/anaconda3/envs/VocalForge/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from VocalForge.audio.audio_utils import create_core_folders\n",
    "create_core_folders(['RawAudio', 'Samples', 'VD', 'Overlap', 'Verification', 'Isolated', 'Exported', 'Noise_Removed', 'Normalized'], workdir=os.path.join(root_path, 'work/audio'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright cool, thats all taken care of. Now for the sake of our demo, we will download a YouTube Playlist of Joe Biden, however you could link your own playlist or simply drop your own local wav files into the `RawAudio` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VocalForge.audio.audio_utils import download_videos\n",
    "\n",
    "download_videos(url='https://www.youtube.com/playlist?list=PLAVNH_8nglubKvZ8bdiEjf9IKKB73SvIy', out_dir=os.path.join(work_path, 'RawAudio'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For actual production, we would want to process all the audio we can get our grubby hands on. But for the purposes of our demo, we will be trimming each audio down to 5 minutes using the `create_samples` method  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VocalForge.audio.audio_utils import create_samples\n",
    "\n",
    "create_samples(\n",
    "    length=300,\n",
    "    input_dir=os.path.join(work_path, 'RawAudio'),\n",
    "    output_dir=os.path.join(work_path, 'Samples'),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voice Activity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the class and set the paths of what the input files are, and where to output the filtered files are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VocalForge.audio import VoiceDetection\n",
    "VD = VoiceDetection(\n",
    "    input_dir=os.path.join(work_path, 'Samples'),\n",
    "    output_dir=os.path.join(work_path, 'VD'),\n",
    ")\n",
    "\n",
    "VD.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! Lets check out the timeline of an audio file to see what parts got deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VD.Timelines[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show audio file ipython\n",
    "from IPython.display import Audio\n",
    "Audio(os.path.join(work_path, 'Samples', 'DATA3.wav'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that the audio highlighted in red has too many short breaks which cause to abrupt cuts in the audio. we can change around some model parameters to change this. by modifying the `min_duration_off` and `min_duration_on` values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HYPER_PARAMETERS = {\n",
    "  # onset/offset activation thresholds\n",
    "  \"onset\": 0.2, \"offset\": 0.6,\n",
    "  # remove speech regions shorter than that many seconds.\n",
    "  \"min_duration_on\": 1.0,\n",
    "  # fill non-speech regions shorter than that many seconds.\n",
    "  \"min_duration_off\": 1.0\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default values are normally:\n",
    "\n",
    "`Onset: 0.5`\n",
    "`Offset: 0.5`\n",
    "`min_duration_on: 0.0`\n",
    "`min_duration_off: 0.0`\n",
    "\n",
    "One can change any of these values to make the values a little more or less liberal in what is speech and what's not (see what I did there?). This can also be used for overlapping speech, however this feature does not exist for isolating voices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VD.Hparams = HYPER_PARAMETERS\n",
    "tuned = VD.analyze_file(os.path.join(work_path, 'Samples', 'DATA3.wav'))\n",
    "tuned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanna go *really* in depth, you could go through each file and change the parameters to get everything as close to perfect as possible. You can then overwrite the default values by doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VD.update_timeline(tuned, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a nicely tuned While this file is nicely edited and doesnt include many periods of non speech, some files may be significantly slimmed down.\n",
    "\n",
    "We can also take a peak into other metrics, which could be used for post processing or fancy dancy post processing you want down the line that I haven't included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total speech duration: 286.35 seconds\n",
      "start: 6.56, end: 66.43\n",
      "start: 68.03, end: 70.36\n",
      "start: 73.35, end: 219.64\n",
      "start: 220.89, end: 294.43\n",
      "start: 295.62, end: 299.94\n",
      "time cut: 13.65 seconds\n"
     ]
    }
   ],
   "source": [
    "from VocalForge.audio.audio_utils import calculate_duration, find_duration_diff, find_original_duration\n",
    "print(f\"total speech duration: {round(calculate_duration(VD.Timestamps[3]), 2)} seconds\")\n",
    "for timestamps in VD.Timestamps[3]:\n",
    "    print(f\"start: {round(timestamps[0], 2)}, end: {round(timestamps[1], 2)}\")\n",
    "print(f\"time cut: {round(find_duration_diff(new_timestamps=VD.Timestamps[3], original_duration=find_original_duration(VD.Input_Files[3])), 2)} seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing The Other Classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will be signiciantly more brief as the process is very similar to the above class, however there are a few things to point out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlap Detection\n",
    "from VocalForge.audio import Overlap\n",
    "\n",
    "OverlapDetection = Overlap(\n",
    "    input_dir=os.path.join(work_path, 'VD'),\n",
    "    output_dir=os.path.join(work_path, 'Overlap')\n",
    ")\n",
    "OverlapDetection.run()\n",
    "print(f\"Time remaining: {round(find_duration_diff(new_timestamps=OverlapDetection.Timestamps[3], original_duration=find_original_duration(OverlapDetection.Input_Files[3])), 2)} seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To seperate speakers and isolate a specific person is a two step process, but in one class for the sake of simplicity. If you only plan to seperate speakers and not to isolate a specific voice, then just don't declare the variables related to the isolating processes.\n",
    "\n",
    "What the is happening here is that the isolating NN iterates through all the files in the `input_dir`, and saves each voice as a seperate wav file in `verification_dir` with the original audio file being now a folder. This can be taken further by isolating a specific voice from the dataset. First, it prompts the user to specify a path to a wav file containing the voice of the speaker. This can be either done through an input() prompt, or specified in the class through the `speaker_id` variable. If you have already calculate the voice data on your own, you can input the info under `speaker_fingerprint`.\n",
    "\n",
    "After that is done and over with, it will iterate through each speaker and compare the speech similaries. The `verification_threshold` is the score it must reach in order to be considered the same person and can be modified as need be. Say that all the speakers in a file does not match the voice enough to reach verification_threshold, the process will repeat by negating 0.05 to verification_threshold until it reaches the `lowest_threshold`, or finds a match. You can disable this feature by simply setting the lowest threshold to the same value as verification_threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rio/anaconda3/envs/VocalForge/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from VocalForge.audio.isolate import Isolate\n",
    "\n",
    "IsolateVoices = Isolate(\n",
    "    input_dir=os.path.join(work_path, 'VD'),\n",
    "    verification_dir=os.path.join(work_path, 'Verification'), #this is where the seperated voices will be saved\n",
    "    export_dir=os.path.join(work_path, 'Isolated'), #this is where the targeted voice will be saved\n",
    "    verification_threshold=0.9, #this is the threshold for two voices to be considered a match (0.00-1.00)\n",
    "    lowest_threshold=0.5, #this is the lowest threshold for a voice to be considered a match if *no* matches are found in the entire file (0.00-1.00)\n",
    "    speaker_id=None,\n",
    "    speaker_fingerprint=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate speakers\n",
    "IsolateVoices.run_separate_speakers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input path to target speaker or specify file through IsolateVoices.Speaker_Id = 'path/to/file'\n",
    "IsolateVoices.run_verification()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to export. This is how we can define the final output of the wav files. \n",
    "\n",
    "By declaring a directory to `noise_removed_dir` will apply deepfilternet2 to each audio file to reduce noise. I find that this specific NN works best compared to solutions like the Adobe Podcast Audio Upscaler for tasks like TTS training or some other application that requires natural audio processing.\n",
    "\n",
    "`normalization_dir`, if declared, will export a copy of the exported audio with normalized audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VocalForge.audio import ExportAudio\n",
    "Exported = ExportAudio(\n",
    "    input_dir=os.path.join(work_path, 'Isolated'),\n",
    "    export_dir=os.path.join(work_path, 'Exported'),\n",
    "    noise_removed_dir=os.path.join(work_path, 'Noise_Removed'),\n",
    "    normalization_dir=os.path.join(work_path, 'Normalized'),\n",
    "    sample_rate=22050,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Noise...\n",
      "\u001b[32m2023-06-03 22:26:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mLoading model settings of DeepFilterNet2\u001b[0m\n",
      "\u001b[32m2023-06-03 22:26:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mUsing DeepFilterNet2 model at /home/rio/.cache/DeepFilterNet/DeepFilterNet2\u001b[0m\n",
      "\u001b[32m2023-06-03 22:26:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mInitializing model `deepfilternet2`\u001b[0m\n",
      "\u001b[32m2023-06-03 22:26:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mFound checkpoint /home/rio/.cache/DeepFilterNet/DeepFilterNet2/checkpoints/model_96.ckpt.best with epoch 96\u001b[0m\n",
      "\u001b[32m2023-06-03 22:26:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on device cuda:0\u001b[0m\n",
      "\u001b[32m2023-06-03 22:26:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mModel loaded\u001b[0m\n",
      "Removing Noise from DATA0.wav...\n",
      "Removing Noise from DATA1.wav...\n",
      "Removing Noise from DATA3.wav...\n",
      "Removing Noise from DATA4.wav...\n",
      "Removing Noise from DATA5.wav...\n",
      "Normalizing Audio...\n"
     ]
    }
   ],
   "source": [
    "Exported.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you're done! Well, sort of. While this process does a pretty good job, to get the best results you will want to check the results manually. As I add more filters, this process will hopefully increase in resolution to reduce the time needed to review the output. But for now, stay vigilent.\n",
    "\n",
    "Next, we will be going over how to format this now refined audio into a dataset ready and prepped for a NN. Stay tuned!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VocalForge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
